{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook chat friend emulator\n",
    "\n",
    "This is an LSTM RNN for producing sentences in the style of a given facebook friend, based on our message history\n",
    "\n",
    "Apologies in advance to my guinea-pig, Dmitri. I censored the ramblings of your robot self to make sure you didn't say anything too outrageous!\n",
    "\n",
    "TO DO : create chat-bot with a message-response sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras module for building LSTM \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "import keras.utils as ku \n",
    "import json\n",
    "import glob\n",
    "from random import sample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of friend to emulate and some hyperparameters\n",
    "friend = 'Dmitri'\n",
    "num_messages = 5000    #randomly sample n messages from the friend's corpus\n",
    "max_length = 20         #trim all messages to this many words. Most messages are short, and longer lengthen the training time significantly\n",
    "lstm_size=100       #Size of the  LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(friend, num_messages, max_length):\n",
    "\n",
    "    PATH_TO_CONV = glob.glob(f'data/messages/inbox/{friend}*/message.json')[0]  #data has weird names but they start with first name so match\n",
    "    \n",
    "    with open(PATH_TO_CONV) as f:\n",
    "        data = json.load(f)\n",
    "                                \n",
    "    data = pd.DataFrame(data['messages'])\n",
    "\n",
    "    def rename(name):\n",
    "        if name=='Simon Roberts':\n",
    "            return 'Me'\n",
    "        else:\n",
    "            return friend\n",
    "        \n",
    "    def trim_message(message):\n",
    "        trimmed = str(message).split(' ')[:max_length]\n",
    "        return ' '.join(trimmed)\n",
    "    \n",
    "    data['sender_name'] = data['sender_name'].apply(rename)   #rename senders to 'Me' and 'First Name'\n",
    "    data['content'] = data[data['content'].apply(type)==str]['content'] #Only use messages which are strings (so just numbers are dropped)\n",
    "    \n",
    "    messages = data[data['sender_name']==friend]['content'].apply(trim_message)  #trim messages to N words\n",
    "\n",
    "    def clean_text(txt):\n",
    "        txt=str(txt)\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "        return txt \n",
    "\n",
    "    return [clean_text(message) for message in sample(list(messages), num_messages)]  #Gets N random messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying some shelves and doors for it tomorrow',\n",
       " 'yeah i saw that',\n",
       " 'are we gonna go for like the final in paris ',\n",
       " 'think it just comes up when they scan your passport',\n",
       " 'exactly']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate the corpus, and look at a few examples\n",
    "corpus = generate_corpus(friend, num_messages, max_length)\n",
    "sample(corpus, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 19, 16)            81056     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               46800     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5066)              511666    \n",
      "=================================================================\n",
      "Total params: 639,522\n",
      "Trainable params: 639,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 16, input_length=input_len))    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(lstm_size))            #Larger vocab probably required larger LSTM layer\n",
    "    model.add(Dropout(0.1))    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30348/30348 [==============================] - 5s 163us/step - loss: 6.8069\n",
      "Epoch 2/50\n",
      "30348/30348 [==============================] - 5s 162us/step - loss: 6.6282\n",
      "Epoch 3/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 6.5610\n",
      "Epoch 4/50\n",
      "30348/30348 [==============================] - 5s 161us/step - loss: 6.5007\n",
      "Epoch 5/50\n",
      "30348/30348 [==============================] - 5s 155us/step - loss: 6.4369\n",
      "Epoch 6/50\n",
      "30348/30348 [==============================] - 5s 169us/step - loss: 6.3630\n",
      "Epoch 7/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 6.2860\n",
      "Epoch 8/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 6.2035\n",
      "Epoch 9/50\n",
      "30348/30348 [==============================] - 5s 158us/step - loss: 6.1201\n",
      "Epoch 10/50\n",
      "30348/30348 [==============================] - 5s 158us/step - loss: 6.0366\n",
      "Epoch 11/50\n",
      "30348/30348 [==============================] - 5s 160us/step - loss: 5.9565\n",
      "Epoch 12/50\n",
      "30348/30348 [==============================] - 5s 155us/step - loss: 5.8784\n",
      "Epoch 13/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 5.8018\n",
      "Epoch 14/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 5.7272\n",
      "Epoch 15/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 5.6538\n",
      "Epoch 16/50\n",
      "30348/30348 [==============================] - 5s 153us/step - loss: 5.5829\n",
      "Epoch 17/50\n",
      "30348/30348 [==============================] - 5s 158us/step - loss: 5.5156\n",
      "Epoch 18/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 5.4515\n",
      "Epoch 19/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 5.3904\n",
      "Epoch 20/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 5.3257\n",
      "Epoch 21/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 5.2668\n",
      "Epoch 22/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 5.2070\n",
      "Epoch 23/50\n",
      "30348/30348 [==============================] - 5s 155us/step - loss: 5.1486\n",
      "Epoch 24/50\n",
      "30348/30348 [==============================] - 5s 159us/step - loss: 5.0875\n",
      "Epoch 25/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 5.0310\n",
      "Epoch 26/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 4.9738\n",
      "Epoch 27/50\n",
      "30348/30348 [==============================] - 5s 153us/step - loss: 4.9139\n",
      "Epoch 28/50\n",
      "30348/30348 [==============================] - 5s 159us/step - loss: 4.8596\n",
      "Epoch 29/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 4.7989\n",
      "Epoch 30/50\n",
      "30348/30348 [==============================] - 5s 158us/step - loss: 4.7422\n",
      "Epoch 31/50\n",
      "30348/30348 [==============================] - 5s 159us/step - loss: 4.6855\n",
      "Epoch 32/50\n",
      "30348/30348 [==============================] - 5s 154us/step - loss: 4.6307\n",
      "Epoch 33/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 4.5726\n",
      "Epoch 34/50\n",
      "30348/30348 [==============================] - 5s 160us/step - loss: 4.5219\n",
      "Epoch 35/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 4.4646\n",
      "Epoch 36/50\n",
      "30348/30348 [==============================] - 5s 154us/step - loss: 4.4124\n",
      "Epoch 37/50\n",
      "30348/30348 [==============================] - 5s 156us/step - loss: 4.3576\n",
      "Epoch 38/50\n",
      "30348/30348 [==============================] - 5s 158us/step - loss: 4.3084\n",
      "Epoch 39/50\n",
      "30348/30348 [==============================] - 5s 158us/step - loss: 4.2567\n",
      "Epoch 40/50\n",
      "30348/30348 [==============================] - 5s 155us/step - loss: 4.2095\n",
      "Epoch 41/50\n",
      "30348/30348 [==============================] - 5s 162us/step - loss: 4.1615\n",
      "Epoch 42/50\n",
      "30348/30348 [==============================] - 5s 157us/step - loss: 4.1169\n",
      "Epoch 43/50\n",
      "30348/30348 [==============================] - 5s 155us/step - loss: 4.0719\n",
      "Epoch 44/50\n",
      "30348/30348 [==============================] - 5s 154us/step - loss: 4.0315\n",
      "Epoch 45/50\n",
      "30348/30348 [==============================] - 5s 160us/step - loss: 3.9916\n",
      "Epoch 46/50\n",
      "30348/30348 [==============================] - 5s 155us/step - loss: 3.9512\n",
      "Epoch 47/50\n",
      "18304/30348 [=================>............] - ETA: 1s - loss: 3.9017"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs=50, verbose=1, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{friend}_{num_messages}messages_{max_length}words_{lstm_size}lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to generate styled sentences based on a seed phrase\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robo-Dmitri: will be home for fifa tonight some weekend sunday is in the pub of least theyve to get to world cup\n",
      "\n",
      "Robo-Dmitri: have you had any of the videos yet the vagina one flapping in the way of years and i got to keep\n",
      "\n",
      "Robo-Dmitri: I dont know how much i will sell for the tv to get fifa out in 20 anyway as they can get\n",
      "\n",
      "Robo-Dmitri: when can have a quick game before u know that youre exactly when it is a bit of the middle i still\n",
      "\n",
      "Robo-Dmitri: obviously just expected to a case of the time together with rooney and van persie being due back from injury soon\n",
      "\n",
      "Robo-Dmitri: I was thinking that sunday is i have all this and all for the last 72 hours p but its i mean have\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's see what he sounds like for some different starting words/phrases!\n",
    "texts = ['will', 'have you', 'I dont', 'when can', 'obviously', 'I was thinking']\n",
    "\n",
    "for text in texts:\n",
    "    print(f'Robo-{friend}: {generate_text(text, 20, model,max_sequence_len)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually sounds a lot like my friend Dmitri!\n",
    "\n",
    "To improve, smileys, standard texts like 'You sent a photo', etc. should either be removed or displayed in their entirety. As it is, we have their artifacts 'p' and 'D' in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
